{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os,sys,inspect\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the folders\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir) # this refers to the project root folder\n",
    "raw_folder = parentdir + \"/data/raw/\"\n",
    "\n",
    "processed_folder = parentdir + \"/data/processed/\"\n",
    "if not os.path.exists(raw_folder):\n",
    "    os.makedirs(raw_folder)\n",
    "    \n",
    "if not os.path.exists(processed_folder):\n",
    "    os.makedirs(processed_folder)\n",
    "\n",
    "combined_file = processed_folder + \"combined_data.csv\"\n",
    "\n",
    "files_to_dl = [\"data.zip\"] # need only this zip file\n",
    "\n",
    "# avoid re-loading the data if the file already exists locally\n",
    "force_download = False # set to True to re-download and unzip the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.84 s, sys: 4.7 s, total: 9.54 s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        if (force_download or not os.path.exists(raw_folder + file[\"name\"])):\n",
    "            os.makedirs(raw_folder, exist_ok=True) # create the folder if not exists\n",
    "            urlretrieve(file[\"download_url\"], raw_folder + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 4.04 s, total: 23.5 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# extract the zip file\n",
    "n_files = len(os.listdir(raw_folder))\n",
    "if (force_download or n_files != 31): # if we must unzip the latest downloaded file or the file was not unzipped\n",
    "    with zipfile.ZipFile(os.path.join(raw_folder, \"data.zip\"), 'r') as f:\n",
    "        f.extractall(raw_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 155.76 MiB, increment: 0.04 MiB\n",
      "CPU times: user 7min 37s, sys: 24.5 s, total: 8min 1s\n",
      "Wall time: 8min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "import pandas as pd\n",
    "use_cols = [\"time\", \"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]\n",
    "files = glob.glob(raw_folder + '*NSW.csv') # exclude observed_daily_rainfall_SYD\n",
    "\n",
    "df = pd.concat((pd.read_csv(file, index_col=0, usecols=use_cols)\n",
    "                .assign(model=file[max(file.rfind('/'), file.rfind('\\\\'))+1:file.index(\"_daily\")])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(combined_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the combined file: 5.560551003552973 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the combined file:\", os.path.getsize(combined_file)/(2**30), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 17.2 s, total: 1min 24s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(combined_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62467843, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min    lat_max   lon_min   lon_max  \\\n",
       "0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "\n",
       "   rain (mm/day)            model  \n",
       "0   4.244226e-13  MPI-ESM-1-2-HAM  \n",
       "1   4.217326e-13  MPI-ESM-1-2-HAM  \n",
       "2   4.498125e-13  MPI-ESM-1-2-HAM  \n",
       "3   4.251282e-13  MPI-ESM-1-2-HAM  \n",
       "4   4.270161e-13  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the data using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# %%memit\n",
    "# dask_combined_file = processed_folder + \"dask_combined_data.csv\"\n",
    "# ddf = dd.read_csv(raw_folder + '*NSW.csv', assume_missing=True, usecols=use_cols, include_path_column=True)\n",
    "# ddf.to_csv(dask_combined_file, single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Size of the combined file:\", os.path.getsize(dask_combined_file)/(2**30), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 682 ms, sys: 164 ms, total: 846 ms\n",
      "Wall time: 905 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min    lat_max   lon_min   lon_max  \\\n",
       "0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "\n",
       "   rain (mm/day)            model  \n",
       "0   4.244226e-13  MPI-ESM-1-2-HAM  \n",
       "1   4.217326e-13  MPI-ESM-1-2-HAM  \n",
       "2   4.498125e-13  MPI-ESM-1-2-HAM  \n",
       "3   4.251282e-13  MPI-ESM-1-2-HAM  \n",
       "4   4.270161e-13  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.read_csv(combined_file)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team have executed the data combination script using Pandas on 4 laptops with the below results.\n",
    "\n",
    "OS|CPU|Memory|Runtime|Memory\n",
    "--|---|------|-------|------\n",
    "macOS BigSur|2.7 GHz Dual-Core Intel Core i5|16 GB|CPU times: user 7min 37s, sys: 24.5 s, total: 8min 1s. Wall time: 8min 35s|peak memory: 155.76 MiB, increment: 0.04 MiB\n",
    "Windows 10 Education Insider Preview|Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz 2.30 GHz|16 GB|Wall time: 8min 55s|peak memory: 138.60 MiB, increment: 0.25 MiB\n",
    "macOS Catalina version 10.15.7|2.4 GHz Quad-Core Intel Core i5|8 GB|CPU times: user 5min 45s, sys: 21.5 s, total: 6min 7s. Wall time: 6min 15s|peak memory: 125.83 MiB, increment: 0.25 MiB\n",
    "Windows 10 Education|Intel(R) Core(TM) i7-8550U CPU @ 1.80 GHz 1.99 GHz| 16 GB|Wall time: 10min 11s|peak memory: 140.70 MiB, increment: 0.20 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the result table, the runtime varied across machines depending on their configurations, but they all took considerable amount of time and memory.\n",
    "\n",
    "Due to the limited hard drive space, we had to comment out the script used for combining data files with Dask, but in the previous execution, using Dask to combine these files actually took more time than using Pandas. Dask's `read_csv` function is, however, super fast in comparision to using Pandas's due to the fact that Pandas loads the whole object into memory whilst Dask does not.\n",
    "\n",
    "In conclusion, merging csv files into one giant file and loading it every time is clearly not an efficient way to work with big data files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
